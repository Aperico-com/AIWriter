{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebe620d-c82a-4662-b024-16af7e0a2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "sethostname: Use the Network Control Panel Applet to set hostname.\n",
      "hostname -s is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 17 11:37:59 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.02       Driver Version: 528.02       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |\n",
      "| 43%   35C    P0    37W / 170W |   1681MiB / 12288MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1328    C+G   C:\\Windows\\System32\\dwm.exe     N/A      |\n",
      "|    0   N/A  N/A      2256    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6872    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A      7488    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A      7844    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n",
      "|    0   N/A  N/A      8848    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     10060    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10164    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11232    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13260    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     13372    C+G   ...ray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     14360    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     14676    C+G   ...3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     16880    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     18024    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     18948    C+G   ...Battle.net\\Battle.net.exe    N/A      |\n",
      "|    0   N/A  N/A     22232    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     24624    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     25444    C+G   ...kyb3d8bbwe\\HxAccounts.exe    N/A      |\n",
      "|    0   N/A  N/A     25476    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/os-release\n",
    "!hostname -I\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9a908-b713-4ddc-b06c-bda7a3e0ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Command:\n",
    "  def __init__(self, title, length, prompt, keywords):\n",
    "    self.title = title\n",
    "    self.length = length\n",
    "    self.prompt = prompt\n",
    "    self.text = \"\"\n",
    "    self.keywords = keywords\n",
    "# class to hold keywords their count and POS tag\n",
    "class AMZKeyword:\n",
    "  def __init__(self, kw):\n",
    "    self.kw = kw\n",
    "    self.count = 1\n",
    "    self.POS = \"NN\"\n",
    "    self.best_review = \"\"\n",
    "    self.in_item_class = 0\n",
    "    self.pos_correct = 0\n",
    "    self.word_count = 0\n",
    "  def clear(self):\n",
    "    self.count = 0\n",
    "\n",
    "commands = []\n",
    "\n",
    "\n",
    "commands.append( Command( \"Best Budget Cocktail Shaker Set\", 160, \"We really like this product from Barillio, it is the best budget cocktail shaker set we have tried.\", [\"super\",\"happy\",\"fantastic\",\"nice\",\"beautiful\",\"pretty\",\"awesome\",\"amazing\",\"perfect\",\"wonderful\",\"professional\",\"best\",\"great\",\"good\",\"excellent\",\"design\",\"quality\",\"cocktail\",\"shaker\"] ) )\n",
    "commands.append( Command( \"\", 160, \"All the pieces in the set are essential and we predict that you are going to make use of them all anytime you are mixing drinks.\", [\"nice\",\"beautiful\",\"pretty\",\"awesome\",\"amazing\",\"perfect\",\"wonderful\",\"professional\",\"best\",\"great\",\"good\",\"excellent\",\"design\",\"quality\",\"shaker\",\"essential\",\"useful\"] ) )\n",
    "commands.append( Command( \"High Quality Budget Cocktail Shaker\", 160, \"Made from rustproof stainless steel, the cocktail shaker from Barillio exudes high quality.\", [\"rust\",\"proof\",\"steel\",\"quality\",\"high\",\"durable\",\"material\",\"style\",\"nice\",\"look\"] ) )\n",
    "commands.append( Command( \"\", 160, \"All the pieces in the set are actually of high quality, stainless steel and BPA free rubber that can be cleaned easily in the dishwasher.\", [\"dish\",\"wash\",\"bpa\",\"rubber\",\"clean\",\"easy\",\"top\",\"happy\"] ) )\n",
    "asin = \"B01L6R2O0O\"\n",
    "special_case = \"True\"\n",
    "domain = \"sites\"\n",
    "rootDomain = \"aperico.com\"\n",
    "item_class = \"cocktail shaker\"\n",
    "item_name = \"cocktail shaker set\"\n",
    "\n",
    "\n",
    "useProxy = True\n",
    "adminUser = 'admin'\n",
    "adminPass = '<password>'\n",
    "banned = {\"bad\",\"disappoint\",\"cheap\", \"poor quality\", \"donâ€™t buy\", \"2-star\", \"2 star\", \"update\", \"low quality\", \"bad quality\", \"do not buy\", \"3-star\", \"3 star\", \"4-star\", \"4 star\"}\n",
    "# training params\n",
    "tune_epochs = 2500\n",
    "tune_print = 50\n",
    "tune_sample = 500\n",
    "tune_save = 500\n",
    "# generation params\n",
    "craziness = 0.95\n",
    "top_k = 50\n",
    "top_p = 0.92\n",
    "about_length = 120\n",
    "summary_length = 160\n",
    "MAX_SUM_LEN = 2800\n",
    "MAX_KEYWORD_SUMMARIES = 5\n",
    "# scraping params\n",
    "limit_reviews = 450\n",
    "min_review_length = 130\n",
    "useEOT = True\n",
    "\n",
    "file_name = \"train.txt\"\n",
    "\n",
    "proxies = {\"http\": \"socks5h://OsqPYlamrJ:4JyIWhccghnX@clt.socks.ipvanish.com:1080\",\n",
    "          \"https\": \"socks5h://OsqPYlamrJ:4JyIWhccghnX@clt.socks.ipvanish.com:1080\"}\n",
    "\n",
    "!pip install -U requests[socks]\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import sys, os\n",
    "\n",
    "headers = {'referrer': 'https://bing.com/', 'User-Agent': '\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0\"'}\n",
    "#headers = {'referrer': 'https://www.google.com', 'User-Agent': '\"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"'}\n",
    "\n",
    "\n",
    "f = open(file_name, \"w+\", encoding='utf-8')\n",
    "print('cwd =', os.getcwd())\n",
    "\n",
    "# Get base training text from own server\n",
    "if special_case:\n",
    "  url = \"https://\" + rootDomain + \"/admin/trainbase.php\";\n",
    "else:\n",
    "  url = \"https://\" + domain + \".\" + rootDomain + \"/admin/trainbase.php\";\n",
    "page = requests.get(url, headers=headers, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "baseTrain = soup.find(id='train-base').text\n",
    "if not useEOT:\n",
    "  baseTrain = baseTrain.replace('<|endoftext|>', '')\n",
    "f.write(baseTrain)\n",
    "\n",
    "# Scrape Amazon product page\n",
    "url = \"https://www.amazon.com/dp/\" + asin\n",
    "if useProxy:\n",
    "  page = requests.get(url, headers=headers, proxies=proxies)\n",
    "else:\n",
    "  page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "#print(soup.text)\n",
    "\n",
    "# Get all features and save to file\n",
    "features = soup.find(id='feature-bullets').find_all(\"li\", {\"class\": \"\"})\n",
    "\n",
    "for feature in features:\n",
    "    feat_text = feature.text.strip()\n",
    "    #while \"  \" in feat_text:\n",
    "        #feat_text = feat_text.replace(\"  \", \" \")\n",
    "    feat_text = \" \".join(feat_text.split())\n",
    "    f.write(feat_text)\n",
    "\n",
    "\n",
    "# Get product description and save to file\n",
    "# Save for later use in summarization if text is long enough\n",
    "desc_text = \"\"\n",
    "desc = soup.find(id='productDescription')\n",
    "if desc:\n",
    "    f.write(desc.text.strip())\n",
    "    desc_text += desc.text.strip().replace('\\n', '').replace('\\t','').replace('\\r','')\n",
    "\n",
    "# Get info text from manufacturer\n",
    "# Save for later use in summarization if text is long enough\n",
    "desc = soup.find('div', class_='aplus-v2 desktop celwidget')\n",
    "if desc:\n",
    "    descDivs = desc.find_all('div', recursive=False)\n",
    "    for dd in descDivs:\n",
    "        f.write(dd.text.strip())\n",
    "        desc_text += dd.text.strip().replace('\\n', '').replace('\\t','').replace('\\r','').replace('Read more','')\n",
    "    \n",
    "desc_text = \"[\" + desc_text + \"]TL;DR:\"\n",
    "#while \"  \" in desc_text:\n",
    "      #desc_text = desc_text.replace(\"  \", \" \")\n",
    "desc_text = \" \".join(desc_text.split())\n",
    "\n",
    "if len(desc_text) > MAX_SUM_LEN:\n",
    "  desc_text = desc_text[0:MAX_SUM_LEN] + \"]TL;DR:\"\n",
    "\n",
    "if useEOT:\n",
    "  f.write('<|endoftext|>')\n",
    "\n",
    "# Get all positive reviews and save to file\n",
    "# Also save longest review for summarization\n",
    "class AMZReview:\n",
    "  def __init__(self, text):\n",
    "    self.text = text\n",
    "    self.length = len(self.text)\n",
    "    self.keywords = []\n",
    "    self.matches = 0\n",
    "    self.used = 0\n",
    "  def clear(self):\n",
    "    self.matches = 0\n",
    "\n",
    "allReviews = []\n",
    "pageCnt = 1\n",
    "reviewCnt = 0\n",
    "tossCnt = 0\n",
    "longest_size = 0\n",
    "longest_review = \"\"\n",
    "second_longest = \"\"\n",
    "third_longest = \"\"\n",
    "url = \"https://www.amazon.com/product-reviews/\" + asin + \"/ref=cm_cr_arp_d_viewpnt_lft?filterByStar=positive&pageNumber=\"\n",
    "\n",
    "while True:\n",
    "    if useProxy:\n",
    "      page = requests.get(url + str(pageCnt), headers=headers, proxies=proxies)\n",
    "    else:\n",
    "      page = requests.get(url + str(pageCnt), headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    reviewsDiv = soup.find(id='cm_cr-review_list')\n",
    "    reviews = reviewsDiv.select(\"div.a-section.celwidget\");\n",
    "    \n",
    "    if len(reviews) <= 0:\n",
    "        break\n",
    "    \n",
    "    for e in reviews:\n",
    "        found_bads = False\n",
    "        tmp_text = e.select(\"div.a-row.a-spacing-small.review-data\")[0].text.lstrip()\n",
    "        if len(tmp_text) > min_review_length:\n",
    "            # check for banned words in text\n",
    "            for bad in banned:\n",
    "              if bad in tmp_text:\n",
    "                found_bads = True\n",
    "                print(\"Found bad word, toss\")\n",
    "                tossCnt += 1\n",
    "                break\n",
    "            if found_bads:\n",
    "              continue\n",
    "            else:\n",
    "              f.write(tmp_text)\n",
    "              reviewCnt += 1\n",
    "              allReviews.append(AMZReview(tmp_text))\n",
    "        else:\n",
    "            tossCnt += 1\n",
    "        # check length and save if longest so far\n",
    "        # make sure it is not too long\n",
    "        if MAX_SUM_LEN < len(e.select(\"div.a-row.a-spacing-small.review-data\")[0].text.lstrip()):\n",
    "          print(\"too long review\")\n",
    "        elif longest_size < len(e.select(\"div.a-row.a-spacing-small.review-data\")[0].text.lstrip()):\n",
    "          third_longest = second_longest\n",
    "          second_longest = longest_review\n",
    "          longest_review = e.select(\"div.a-row.a-spacing-small.review-data\")[0].text.lstrip()\n",
    "          longest_size = len(e.select(\"div.a-row.a-spacing-small.review-data\")[0].text.lstrip())\n",
    "\n",
    "    #print(\"Longest single review is \" + str(longest_size))\n",
    "    \n",
    "\n",
    "    # don't fetch any more pages if max limit of reviews is reached\n",
    "    if reviewCnt > limit_reviews:\n",
    "          break\n",
    "        \n",
    "    pageCnt = pageCnt + 1\n",
    "\n",
    "print(\"Tossed reviews: \" + str(tossCnt))\n",
    "print(\"Kept reviews: \" + str(reviewCnt))\n",
    "\n",
    "# add top 3 reviews together and use for in conclusion summarization\n",
    "if (len(longest_review) + len(second_longest) + len(third_longest) <= MAX_SUM_LEN):\n",
    "    longest_review += \" \" + second_longest + \" \" + third_longest\n",
    "elif (len(longest_review) + len(second_longest)) <= MAX_SUM_LEN:\n",
    "    longest_review += \" \" + second_longest\n",
    "elif (len(longest_review) + len(third_longest)) <= MAX_SUM_LEN:\n",
    "    longest_review += \" \" + third_longest\n",
    "\n",
    "\n",
    "longest_review = longest_review.strip().replace('\\n', '').replace('\\t','').replace('\\r','')\n",
    "longest_review = \"[\" + longest_review + \"]\" + \"TL;DR:\"\n",
    "\n",
    "# use YAKE to get keywords for all reviews select the ones that match our keywords and \n",
    "# update the text field for all Command objects\n",
    "# use nltk for stemming and POS tagging\n",
    "!pip install git+https://github.com/LIAAD/yake\n",
    "import yake\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 30\n",
    "# hold all keywords and their frequenzy across all reviews\n",
    "allKeywords = dict()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "kwExtractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "\n",
    "for r in allReviews:\n",
    "    r.keywords = dict(kwExtractor.extract_keywords(r.text))\n",
    "    # add all keywords and their freq to dict\n",
    "    for kw in r.keywords:\n",
    "        if allKeywords.get(kw):\n",
    "            allKeywords[kw].count += 1\n",
    "            # add to best matching review for this keyword if it does not take string to over limit\n",
    "            if ( len(allKeywords[kw].best_review) + len(r.text) ) < MAX_SUM_LEN:\n",
    "              allKeywords[kw].best_review += r.text\n",
    "        else:\n",
    "            allKeywords[kw] = AMZKeyword(kw)\n",
    "            # add this review to this keyword\n",
    "            if len(r.text) < MAX_SUM_LEN:\n",
    "              allKeywords[kw].best_review = r.text\n",
    "            \n",
    "# sort dict to list most frequent first\n",
    "sortedList = sorted(allKeywords.values(), key=lambda kw: kw.count, reverse=True)\n",
    "\n",
    "# setup to post to database on server via requests\n",
    "keyword_rev = \"\"\n",
    "kw_count = 0\n",
    "total_rev = len(allReviews)\n",
    "percent_rev = 0.0\n",
    "in_item_class = 0\n",
    "\n",
    "if special_case:\n",
    "  url = \"https://\" + rootDomain + \"/admin/keywords.php\"\n",
    "else:\n",
    "  url = \"https://\" + domain + \".\" + rootDomain + \"/admin/keywords.php\"\n",
    "\n",
    "# go through list look for POS patterns (JJx|RBS|RBR) : (IN|TO) : (VBx|NNx)\n",
    "for x in sortedList:\n",
    "    \n",
    "    in_item_class = 0\n",
    "\n",
    "    # tokenize and POS tag\n",
    "    kw_tokens = word_tokenize(x.kw)\n",
    "    item_class_tokens = word_tokenize(item_name)\n",
    "    tags = nltk.pos_tag(kw_tokens)\n",
    "    # stem both arrays for accurate matching later\n",
    "    kw_stem = []\n",
    "    item_class_stem = []\n",
    "    for w in kw_tokens:\n",
    "        kw_stem.append(stemmer.stem(w))\n",
    "    for w in item_class_tokens:\n",
    "        item_class_stem.append(stemmer.stem(w))\n",
    "    \n",
    "    # if keyword matches any of the item class words, skip it\n",
    "    if len(list(set(kw_stem) & set(item_class_stem))) > 0:\n",
    "        in_item_class = 1\n",
    "        x.in_item_class = 1\n",
    "    \n",
    "    # test POS pattern\n",
    "    if (tags[0][1][0:2] == 'JJ' or tags[0][1] == 'RBS'  or tags[0][1] == 'RBR') and x.count >= 5:\n",
    "        if len(tags) == 1:\n",
    "            print(x.kw +\": \"+ str(x.count))\n",
    "            print(tags)\n",
    "            obj = {'len': len(tags),'keyword': x.kw, 'count': x.count, 'asin': asin, 'total': total_rev, 'percent': (x.count/total_rev), 'itemclass': in_item_class}\n",
    "            r = requests.post(url, data = obj)\n",
    "            x.pos_correct = 1\n",
    "            x.word_count = 1\n",
    "            print(r.text)\n",
    "        elif len(tags) == 2:\n",
    "            if (tags[1][1][0:2] == 'NN' or tags[1][1][0:2] == 'VB'):\n",
    "                print(x.kw +\": \"+ str(x.count))\n",
    "                print(tags)\n",
    "                obj = {'len': len(tags), 'keyword': x.kw, 'count': x.count, 'asin': asin, 'total': total_rev, 'percent': (x.count/total_rev), 'itemclass': in_item_class}\n",
    "                r = requests.post(url, data = obj)\n",
    "                x.pos_correct = 1\n",
    "                x.word_count = 2\n",
    "                print(r.text)\n",
    "        elif len(tags) == 3:\n",
    "            if (tags[1][1] == 'IN' or tags[1][1] == 'TO') and (tags[2][1][0:2] == 'VB' or tags[2][1][0:2] == 'NN'):\n",
    "                print(x.kw +\": \"+ str(x.count))\n",
    "                print(tags)\n",
    "                obj = {'len': len(tags), 'keyword': x.kw, 'count': x.count, 'asin': asin, 'total': total_rev, 'percent': (x.count/total_rev), 'itemclass': in_item_class}\n",
    "                r = requests.post(url, data = obj)\n",
    "                x.pos_correct = 1\n",
    "                x.word_count = 3\n",
    "                print(r.text)\n",
    "\n",
    "for command in commands:\n",
    "\n",
    "  summaryText = \"\"\n",
    "  matches = command.keywords\n",
    "  newList = allReviews\n",
    "\n",
    "  # reset matches count between prompt\n",
    "  for nl in newList:\n",
    "    nl.clear()\n",
    "\n",
    "  # go through all reviews\n",
    "  # for all keywords in review\n",
    "  # match against all keywords in prompt\n",
    "  for r in newList:\n",
    "      for key in r.keywords:\n",
    "        for match in matches:\n",
    "          if match in key:\n",
    "              r.matches += key.count(match)\n",
    "\n",
    "  newList.sort(key=lambda x: x.matches, reverse=True)\n",
    "\n",
    "  # add highest match count reviews to summary string until max length is reached          \n",
    "  for r in newList:\n",
    "      if r.matches < 1:\n",
    "          break\n",
    "      #else:\n",
    "          #print(\"Match score in review: \" + str(r.matches))\n",
    "\n",
    "      if r.used == 1:\n",
    "        print(\"Skipping review, already used (\" + str(r.matches) +\")\")\n",
    "        continue\n",
    "\n",
    "      # don't go over limit, check if next text is short enough\n",
    "      if (len(summaryText) + len(r.text)) >= MAX_SUM_LEN:\n",
    "          continue\n",
    "      else:\n",
    "          summaryText += \" \" + r.text\n",
    "          r.used = 1\n",
    "          print(\"Adding review (\" + str(r.matches) +\")\")\n",
    "        \n",
    "  summaryText = summaryText.strip().replace('\\n', '').replace('\\t','').replace('\\r','')\n",
    "  #while \"  \" in summaryText:\n",
    "      #summaryText = summaryText.replace(\"  \", \" \")\n",
    "  summaryText = \" \".join(summaryText.split())\n",
    "\n",
    "  command.text = \"[\" + summaryText + \"]\" + \"TL;DR:\"\n",
    "        \n",
    "  print(\"Summary text [\" + str(len(summaryText)) + \"] : \" + summaryText)\n",
    "\n",
    "\n",
    "%tensorflow_version 1.x\n",
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "gpt2.download_gpt2(model_name=\"355M\")\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='355M',\n",
    "              steps=tune_epochs,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=tune_print,\n",
    "              sample_every=tune_sample,\n",
    "              save_every=tune_save\n",
    "              )\n",
    "\n",
    "from gpt_2_simple.src import encoder\n",
    "enc = encoder.get_encoder(\"checkpoint/run1\")\n",
    "\n",
    "# generate 5 texts for each command\n",
    "for command in commands:\n",
    "  context_tokens = enc.encode(command.text)\n",
    "  print(\"TOKENS:\" + str(len(context_tokens)) + \" | LEN:\" + str(command.length))\n",
    "  samples = []\n",
    "  samples = gpt2.generate(sess,\n",
    "              length=command.length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=command.text,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "  \n",
    "  samplecount = 0\n",
    "  for sample in samples:\n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if len(command.prompt) > 3:\n",
    "      aiPrompt = command.prompt + \" \" + sample[sample.index(\"TL;DR:\")+6:]\n",
    "    else:\n",
    "      aiPrompt = sample[sample.index(\"TL;DR:\")+6:]\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': command.title, 'text': aiPrompt, 'asin': asin, 'active': '1', 'type': 0}\n",
    "    else:\n",
    "      obj = {'title': command.title, 'text': aiPrompt, 'asin': asin, 'active': '0', 'type': 0}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    print(r.text)\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "\n",
    "  # if the command has a non empty prompt generate text from it, not TL;DR:\n",
    "  if len(command.prompt) > 3:\n",
    "    samples = gpt2.generate(sess,\n",
    "              length=command.length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=command.prompt,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "  \n",
    "    samplecount = 0\n",
    "    for sample in samples:\n",
    "      if special_case:\n",
    "        url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "      else:\n",
    "        url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "      if samplecount == 0:\n",
    "        obj = {'title': command.title, 'text': sample, 'asin': asin, 'active': '1', 'type': 0}\n",
    "      else:\n",
    "        obj = {'title': command.title, 'text': sample, 'asin': asin, 'active': '0', 'type': 0}\n",
    "\n",
    "      r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "      print(r.text)\n",
    "      samplecount = samplecount + 1\n",
    "      if samplecount == 5:\n",
    "        samplecount = 0\n",
    "\n",
    "# generate 5 texts for extracted key phrase, top MAX_KEYWORD_SUMMARIES results only\n",
    "tmp_cnt = 0\n",
    "for x in sortedList:\n",
    "\n",
    "  if x.in_item_class == 1 or x.pos_correct == 0 or x.word_count < 2:\n",
    "    print(\"keyword in item class, too short or POS incorrect: \" + x.kw)\n",
    "    continue\n",
    "  else:\n",
    "    print(\"using this keyword: \" + x.kw + \"count=\" + str(x.count))\n",
    "    tmp_cnt = tmp_cnt + 1\n",
    "\n",
    "  if tmp_cnt > MAX_KEYWORD_SUMMARIES:\n",
    "    break\n",
    "\n",
    "  context_tokens = enc.encode(x.best_review)\n",
    "  print(\"TOKENS:\" + str(len(context_tokens)))\n",
    "  samples = []\n",
    "  tmp_prompt = \"[\" + x.best_review +\"]TL;DR:\"\n",
    "  samples = gpt2.generate(sess,\n",
    "              length=140,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=tmp_prompt,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "  \n",
    "  samplecount = 0\n",
    "  for sample in samples:\n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    aiPrompt = sample[sample.index(\"TL;DR:\")+6:]\n",
    "\n",
    "    tmp_title = x.kw.title() + \" \" + item_name.title()\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': tmp_title, 'text': aiPrompt, 'asin': asin, 'active': '1', 'type': 0}\n",
    "    else:\n",
    "      obj = {'title': tmp_title, 'text': aiPrompt, 'asin': asin, 'active': '0', 'type': 0}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    print(r.text)\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "\n",
    "# generate summary for features + description \"about\"\n",
    "print(\"Desc is \" + str(len(desc_text)) + \" characters.\")\n",
    "print(desc_text)\n",
    "print(\"=\" * 120)\n",
    "\n",
    "summaries = gpt2.generate(sess,\n",
    "              length=about_length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=desc_text,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "\n",
    "samplecount = 0\n",
    "for summary in summaries:\n",
    "    print(summary[summary.index(\"TL;DR:\")+6:])\n",
    "    print(\"-\"  * 120)\n",
    "\n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': 'About', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '1', 'type': 1}\n",
    "    else:\n",
    "      obj = {'title': 'About', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '0', 'type': 1}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "\n",
    "# generate summary for longest review\n",
    "print(\"Longest summary is \" + str(len(longest_review)) + \" characters.\")\n",
    "print(\"1:\" + longest_review)\n",
    "print(\"2:\" + second_longest)\n",
    "print(\"3:\" + third_longest)\n",
    "print(\"=\" * 120)\n",
    "\n",
    "summaries = gpt2.generate(sess,\n",
    "              length=summary_length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=longest_review,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "\n",
    "samplecount = 0\n",
    "for summary in summaries:\n",
    "    print(summary[summary.index(\"TL;DR:\")+6:])\n",
    "    print(\"-\"  * 120)\n",
    "    \n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': 'In Conclusion', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '1', 'type': 2}\n",
    "    else:\n",
    "      obj = {'title': 'In Conclusion', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '0', 'type': 2}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc3823-9757-4ca2-a11a-8160ba5b9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 5 texts for each command\n",
    "for command in commands:\n",
    "  context_tokens = enc.encode(command.text)\n",
    "  print(\"TOKENS:\" + str(len(context_tokens)) + \" | LEN:\" + str(command.length))\n",
    "  samples = []\n",
    "  samples = gpt2.generate(sess,\n",
    "              length=command.length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=command.text,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "  \n",
    "  samplecount = 0\n",
    "  for sample in samples:\n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if len(command.prompt) > 3:\n",
    "      aiPrompt = command.prompt + \" \" + sample[sample.index(\"TL;DR:\")+6:]\n",
    "    else:\n",
    "      aiPrompt = sample[sample.index(\"TL;DR:\")+6:]\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': command.title, 'text': aiPrompt, 'asin': asin, 'active': '1', 'type': 0}\n",
    "    else:\n",
    "      obj = {'title': command.title, 'text': aiPrompt, 'asin': asin, 'active': '0', 'type': 0}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    print(r.text)\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "\n",
    "  # if the command has a non empty prompt generate text from it, not TL;DR:\n",
    "  if len(command.prompt) > 3:\n",
    "    samples = gpt2.generate(sess,\n",
    "              length=command.length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=command.prompt,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "  \n",
    "    samplecount = 0\n",
    "    for sample in samples:\n",
    "      if special_case:\n",
    "        url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "      else:\n",
    "        url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "      if samplecount == 0:\n",
    "        obj = {'title': command.title, 'text': sample, 'asin': asin, 'active': '1', 'type': 0}\n",
    "      else:\n",
    "        obj = {'title': command.title, 'text': sample, 'asin': asin, 'active': '0', 'type': 0}\n",
    "\n",
    "      r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "      print(r.text)\n",
    "      samplecount = samplecount + 1\n",
    "      if samplecount == 5:\n",
    "        samplecount = 0\n",
    "\n",
    "# generate 5 texts for extracted key phrase, top MAX_KEYWORD_SUMMARIES results only\n",
    "tmp_cnt = 0\n",
    "for x in sortedList:\n",
    "\n",
    "  if x.in_item_class == 1 or x.pos_correct == 0 or x.word_count < 2:\n",
    "    print(\"keyword in item class, too short or POS incorrect: \" + x.kw)\n",
    "    continue\n",
    "  else:\n",
    "    print(\"using this keyword: \" + x.kw + \"count=\" + str(x.count))\n",
    "    tmp_cnt = tmp_cnt + 1\n",
    "\n",
    "  if tmp_cnt > MAX_KEYWORD_SUMMARIES:\n",
    "    break\n",
    "\n",
    "  context_tokens = enc.encode(x.best_review)\n",
    "  print(\"TOKENS:\" + str(len(context_tokens)))\n",
    "  samples = []\n",
    "  tmp_prompt = \"[\" + x.best_review +\"]TL;DR:\"\n",
    "  samples = gpt2.generate(sess,\n",
    "              length=140,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=tmp_prompt,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "  \n",
    "  samplecount = 0\n",
    "  for sample in samples:\n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    aiPrompt = sample[sample.index(\"TL;DR:\")+6:]\n",
    "\n",
    "    tmp_title = x.kw.title() + \" \" + item_name.title()\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': tmp_title, 'text': aiPrompt, 'asin': asin, 'active': '1', 'type': 0}\n",
    "    else:\n",
    "      obj = {'title': tmp_title, 'text': aiPrompt, 'asin': asin, 'active': '0', 'type': 0}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    print(r.text)\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "\n",
    "# generate summary for features + description \"about\"\n",
    "print(\"Desc is \" + str(len(desc_text)) + \" characters.\")\n",
    "print(desc_text)\n",
    "print(\"=\" * 120)\n",
    "\n",
    "summaries = gpt2.generate(sess,\n",
    "              length=about_length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=desc_text,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "\n",
    "samplecount = 0\n",
    "for summary in summaries:\n",
    "    print(summary[summary.index(\"TL;DR:\")+6:])\n",
    "    print(\"-\"  * 120)\n",
    "\n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': 'About', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '1', 'type': 1}\n",
    "    else:\n",
    "      obj = {'title': 'About', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '0', 'type': 1}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "\n",
    "# generate summary for longest review\n",
    "print(\"Longest summary is \" + str(len(longest_review)) + \" characters.\")\n",
    "print(\"1:\" + longest_review)\n",
    "print(\"2:\" + second_longest)\n",
    "print(\"3:\" + third_longest)\n",
    "print(\"=\" * 120)\n",
    "\n",
    "summaries = gpt2.generate(sess,\n",
    "              length=summary_length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=longest_review,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "\n",
    "samplecount = 0\n",
    "for summary in summaries:\n",
    "    print(summary[summary.index(\"TL;DR:\")+6:])\n",
    "    print(\"-\"  * 120)\n",
    "    \n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': 'In Conclusion', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '1', 'type': 2}\n",
    "    else:\n",
    "      obj = {'title': 'In Conclusion', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '0', 'type': 2}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810812e1-968a-4d65-9321-e3f422ef3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summary for features + description \"about\"\n",
    "desc_text += \"]TL;DR:\"\n",
    "print(\"Desc is \" + str(len(desc_text)) + \" characters.\")\n",
    "print(desc_text)\n",
    "print(\"=\" * 120)\n",
    "\n",
    "summaries = gpt2.generate(sess,\n",
    "              length=about_length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=desc_text,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "\n",
    "samplecount = 0\n",
    "for summary in summaries:\n",
    "    print(summary[summary.index(\"TL;DR:\")+6:])\n",
    "    print(\"-\"  * 120)\n",
    "\n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': 'About', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '1', 'type': 1}\n",
    "    else:\n",
    "      obj = {'title': 'About', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '0', 'type': 1}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0\n",
    "\n",
    "# generate summary for longest review\n",
    "print(\"Longest summary is \" + str(len(longest_review)) + \" characters.\")\n",
    "print(\"1:\" + longest_review)\n",
    "print(\"2:\" + second_longest)\n",
    "print(\"3:\" + third_longest)\n",
    "print(\"=\" * 120)\n",
    "\n",
    "summaries = gpt2.generate(sess,\n",
    "              length=summary_length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=longest_review,\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              top_k = top_k,\n",
    "              top_p = top_p\n",
    "              )\n",
    "\n",
    "samplecount = 0\n",
    "for summary in summaries:\n",
    "    print(summary[summary.index(\"TL;DR:\")+6:])\n",
    "    print(\"-\"  * 120)\n",
    "    \n",
    "    if special_case:\n",
    "      url = 'https://' + rootDomain + '/admin/asin.php'\n",
    "    else:\n",
    "      url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': 'In Conclusion', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '1', 'type': 2}\n",
    "    else:\n",
    "      obj = {'title': 'In Conclusion', 'text': summary[summary.index(\"TL;DR:\")+6:], 'asin': asin, 'active': '0', 'type': 2}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f10972-a073-4dcf-8bdd-1037dbbc93e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use below to train, generate and upload IF RUNTIME WAS RESTARTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63d64c-e2a5-40d2-b32f-2bdb922e95ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Command:\n",
    "  def __init__(self, title, length, prompt):\n",
    "    self.title = title\n",
    "    self.length = length\n",
    "    self.prompt = prompt\n",
    "\n",
    "commands = []\n",
    "\n",
    "commands.append( Command( \"Best UV Blacklight\", 160, \"The Escolite UV Blacklight is certainly a contender for the best UV blacklight title.\" ) )\n",
    "commands.append( Command( \"\", 150, \"After testing this UV blacklight extensively we think this is the perfect and most well rounded UV blacklight on the market.\" ) )\n",
    "commands.append( Command( \"Effective UV Blacklight\", 200, \"It is incredibly effective to use the Escolite UV Blacklight to find stains that are invisible to the naked eye.\" ) )\n",
    "commands.append( Command( \"\", 140, \"You might have some problems finding stains of cat urine in particular since it is not fluorescent.\" ) )\n",
    "commands.append( Command( \"UV Blacklight Scorpion Finder\", 160, \"The Escolite UV Blacklight is also perfect for finding scorpions on the ground.\" ) )\n",
    "commands.append( Command( \"\", 150, \"You will be able to see even tiny scorpions and this works even when it is dark outside.\" ) )\n",
    "asin = \"B008133KB4\"\n",
    "\n",
    "\n",
    "domain = \"flashlights\"\n",
    "rootDomain = \"junglesentry.com\"\n",
    "adminUser = 'admin'\n",
    "adminPass = 'aZn2iDbkS3iY'\n",
    "\n",
    "tune_epochs = 2000\n",
    "tune_print = 50\n",
    "tune_sample = 200\n",
    "tune_save = 500\n",
    "craziness = 1.0\n",
    "limit_reviews = 500\n",
    "min_review_length = 120\n",
    "useEOT = True\n",
    "\n",
    "file_name = \"train.txt\"\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import sys, os\n",
    "%tensorflow_version 1.x\n",
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "#gpt2.mount_gdrive()\n",
    "\n",
    "gpt2.download_gpt2(model_name=\"355M\")\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='355M',\n",
    "              steps=tune_epochs,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=tune_print,\n",
    "              sample_every=tune_sample,\n",
    "              save_every=tune_save\n",
    "              )\n",
    "\n",
    "for command in commands:\n",
    "  samples = []\n",
    "  samples = gpt2.generate(sess,\n",
    "              length=command.length,\n",
    "              return_as_list=True,\n",
    "              temperature=craziness,\n",
    "              prefix=command.prompt,\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )\n",
    "  \n",
    "  samplecount = 0\n",
    "  for sample in samples:\n",
    "    url = 'https://' + domain + '.' + rootDomain + '/admin/asin.php'\n",
    "    if samplecount == 0:\n",
    "      obj = {'title': command.title, 'text': sample, 'asin': asin, 'active': '1'}\n",
    "    else:\n",
    "      obj = {'title': command.title, 'text': sample, 'asin': asin, 'active': '0'}\n",
    "\n",
    "    r = requests.post(url, data = obj, auth=HTTPBasicAuth(adminUser, adminPass))\n",
    "\n",
    "    print(r.text)\n",
    "    samplecount = samplecount + 1\n",
    "    if samplecount == 5:\n",
    "      samplecount = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
